%! Author = Kevin Lin
%! Date = 1/30/2026

% Preamble
\documentclass[11pt,a4paper,margin=1in]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{pdfpages}

\title{HW 3}
\author{Kevin Lin}
\date{1/30/2026}

% Document
\begin{document}
\maketitle

\section{}
Let $X \sim \text{Unif}[-1,1], Z \sim \mathbb{N}(0,1), Y=XZ^2$, independently.
\begin{enumerate}[(i)]
    \item We can find the CEF $E[Y|X]$ as follows:
    \begin{align*}
        E[Y|X] &= E[XZ^2|X] \\
        &= X E[Z^2|X] \\
        &= X E[Z^2] \\
        &= X \cdot \text{Var}(Z) \\
        &= X \cdot 1 \\
        &= X
    \end{align*}
    The conditional variance $\text{Var}(Y|X)$ is:
    \begin{align*}
        \text{Var}(Y|X) &= \text{Var}(XZ^2|X) \\
        &= X^2 \text{Var}(Z^2|X) \\
        &= X^2 \text{Var}(Z^2) \\
        &= X^2 (E[Z^4] - (E[Z^2])^2) \\
        &= X^2 (3 - 1) \\
        &= 2X^2
    \end{align*}
    \item We can find the best linear projection of the CEF $\beta_0 + \beta_1 X$ 
    as follows:
    \begin{align*}
        \beta_1 &= \frac{\text{Cov}(Y,X)}{\text{Var}(X)} \\
        &= \frac{E[YX] - E[Y]E[X]}{\text{Var}(X)} \\
        &= \frac{E[X^2 Z^2] - 0 \cdot 0}{\text{Var}(X)} \\
        &= \frac{E[X^2] E[Z^2]}{\text{Var}(X)} \\
        &= \frac{\frac{1}{3} \cdot 1}{\frac{1}{3}} \\
        &= 1 \\
        \beta_0 &= E[Y] - \beta_1 E[X] \\
        &= 0 - 1 \cdot 0 \\
        &= 0
    \end{align*}
    Given that the CEF is $E[Y|X] = X$, the linear projection perfectly matches the CEF.
    \item If we observe $n$ i.i.d samples of $(X_i, Y_i)$, the asymptotic variance
    of the fitted coefficients $\hat{\beta}_0, \hat{\beta}_1$ are given by:
    \begin{align*}
        AV(\hat{\beta}) &= (E[XX'])^{-1} E[\text{Var}(Y|X) XX'] (E[XX'])^{-1} \\
        &= \begin{pmatrix}
            1 & 0 \\
            0 & 3
        \end{pmatrix}
        \begin{pmatrix}
            E[\text{Var}(Y|X)] & E[X \text{Var}(Y|X)] \\
            E[X \text{Var}(Y|X)] & E[X^2 \text{Var}(Y|X)]
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 \\
            0 & 3
        \end{pmatrix} \\
        &= \begin{pmatrix}
            E[\text{Var}(Y|X)] & 3 E[X \text{Var}(Y|X)] \\
            3 E[X \text{Var}(Y|X)] & 9 E[X^2 \text{Var}(Y|X)]
        \end{pmatrix} \\
        &= \begin{pmatrix}
            E[2X^2] & 3 E[X \cdot 2X^2] \\
            3 E[X \cdot 2X^2] & 9 E[X^2 \cdot 2X^2]
        \end{pmatrix} \\
        &= \begin{pmatrix}
            2/3 & 0 \\
            0 & 18/5
        \end{pmatrix}
    \end{align*}
    Consequently with $n$ i.i.d samples, the variances are:
    \begin{align*}
        AV(\hat{\beta}_0) &= \frac{2}{3n} \\
        AV(\hat{\beta}_1) &= \frac{18}{5n}
    \end{align*}
    \item Under the standard assumptions, we would assume that $Var(Y|X) = \sigma^2$,
    which is a constant. Consequently, the asymptotic variance of the fitted coefficients
    would be:
    \begin{align*}
        AV(\hat{\beta}) &= (E[XX'])^{-1} E[\text{Var}(Y|X) XX'] (E[XX'])^{-1} \\
        &= \sigma^2 (E[XX'])^{-1} \\
        &= \sigma^2
        \begin{pmatrix}
            1 & 0 \\
            0 & 3
        \end{pmatrix}
    \end{align*}
    We can estimate $\sigma^2$ with $E[\text{Var}(Y|X)]$ as follows:
    \begin{align*}
        \sigma^2 &\approx E[\text{Var}(Y|X)] \\
        &= E[2X^2] \\
        &= \frac{2}{3}
    \end{align*}
    Thus, the asymptotic variances would be:
    \begin{align*}
        AV(\hat{\beta}_0) &\approx \frac{2}{3n} \\
        AV(\hat{\beta}_1) &\approx \frac{2}{n} \\
    \end{align*} 
    The discrepancy in $AV(\hat{\beta}_1)$ arises because the standard assumption
    of homoskedasticity is violated in this case since $\text{Var}(Y|X)$ is not constant.
    The variance $\text{Var}(Y|X)$ grows quadratically with $X$, leading to a 
    larger asymptotic variance which affects OLS disproportionately. Interestingly,
    by chance, the estimate for $AV(\hat{\beta}_0)$ remains accurate due to the
    symmetry of $X$ around 0, which causes the heteroskedasticity to have a
    negligible effect on the intercept term.
\end{enumerate}

\section{}
\begin{enumerate}[(i)]
    \item See attached Jupyter notebook for code and plots.
    \item The sandwich bands are noticeably wider in each set of pointwise and 
    simultaneous bands. This is because the sandwich bands account for potential
    heteroskedasticity in the error terms, which increases the uncertainty in the
    estimates. In contrast, the standard bands assume homoskedasticity, which can
    lead to narrower bands that may underestimate the true variability of the estimates.
    In general as well, the Bonferroni simultaneous bands are wider than the pointwise
    bands, since they account for the multiple comparisons problem by adjusting the
    confidence level for each individual interval to maintain an overall confidence. 
    We can also see that the confidence bands are much wider on the right, as due 
    to our data generating following $y=2^x$, the variance of $Y$ increases
    exponentially with $X$. This leads to greater uncertainty in the estimates as
    the OLS extrapolates badly at larger values of $x$.
\end{enumerate}

\section{}
\begin{enumerate}[(i)]
    \item See attached Jupyter notebook for code and plots. Immediately we see that
    coverage is terrible because the true regression line is non-linear, while
    OLS is fitting a linear model. Consequently, the confidence bands will rarely
    cover the true regression line at all $X_i$. Similar to problem 2, we see that
    the sandwich bands are wider than the standard bands, and the simultaneous
    bands are wider than the pointwise bands. Likewise, we see that coverage
    gets worse as $X$ increases due to the exponential increase in variance of $Y$.
    \item See attached Jupyter notebook for code. We see that none of the pointwise
    confidence bands contain the true CEF at all $X_i$, while only 3 out of 1000
    simultaneous confidence bands contain the true CEF at all $X_i$. This poor 
    coverage is due to the fact that the true CEF is non-linear, while OLS is
    fitting a linear model, as explained in (i). Mathematically, the confidence
    bands assume that $\text{estimator} - \text{target} = \text{error}$, however
    because we have a $x^3$ term in the true CEF, the bias is ommitted from the 
    equation, leading to poor coverage. Even as $n$ increases and the sampling error
    decreases, the bias remains, leading to persistent undercoverage. This even occurs
    with the sandwich bonds, as robustness only handles heteroskedasticity, not model
    misspecification and bias.
    \item In a pointwise coverage, we can see that this statistic is much better, 
    however no CI reaches the nominal 95\% coverage. This is because while
    pointwise coverage is easier to achieve than simultaneous coverage, the bias
    from model misspecification still leads to undercoverage. Even as $n$ increases,
    the bias remains, leading to persistent undercoverage. The Bonferroni bands 
    only improve coverage because they are wider due to the higher variance.
\end{enumerate}

\includepdf[pages=-]{hw3_code.pdf}

\end{document}